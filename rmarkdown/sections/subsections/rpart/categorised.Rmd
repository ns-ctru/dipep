##### Pre-Categorised Continuous Variables

```{r results_rpart_categorical, echo = FALSE, cache = FALSE, results = 'hide', eval = TRUE}
## Useful reading...
##
## http://gormanalysis.com/decision-trees-in-r-using-rpart/
categorical <- dipep_rpart(df             = dipep,
                           predictor      = c('age.cat', 'smoking', 'temperature.cat', 'bp.diastolic.cat', 'bp.systolic.cat',
                                              'o2.saturation.cat', 'respiratory.rate.cat', 'bmi.cat',
                                              'pregnancies.under.cat', 'pregnancies.over.cat', 'prev.preg.problem',
                                              'presenting.features.pleuritic',
                                              'presenting.features.non.pleuritic', 'presenting.features.sob.exertion',
                                              'presenting.features.sob.rest', 'presenting.features.haemoptysis',
                                              'presenting.features.cough', 'presenting.features.syncope',
                                              'presenting.features.palpitations', 'presenting.features.other',
                                              'surgery', 'cesarean', 'thromb.event',
                                              'thromboprophylaxis', 'thrombosis', 'preg.post', 'num.fetus'),
                           exclude.non.recruited = TRUE,
                           exclude.dvt           = TRUE,
                           legend                = FALSE,
                           threshold             = 0.5,
                           rpart.opts.method     = 'class',
                           rpart.opts.minsplit   = 4,
                           rpart.opts.minbucket  = 2,
                           rpart.opts.cp         = -1)

```

This section shows the results from applying recursive partitioning to pre-categorised continuous variables.  Leave One Out Cross-Validation (LOOCV) is performed automatically by the `rpart` package at each partition and the resulting estimates used in summarising and prunning the tree.



The [dendrogram](https://en.wikipedia.org/wiki/Dendrogram) below shows the full, over-fitted.  As its over-fitted there are a lot of splits and the graph is barely legible.

```{r results_rpart_categorical_plot, echo = FALSE, cache = FALSE, fig_width = 15, fig_height = 10, eval = TRUE}
prp(categorical$rpart.full,
    type        = prp.opts$type,
    extra       = prp.opts$extra,
    box.palette = prp.opts$box.palette,
    yesno       = prp.opts$yesno,
    branch      = prp.opts$branch,
    varlen      = prp.opts$varlen,
    faclen      = prp.opts$faclen)

```


This model, because it was forced to fit a full model that categorised everyone, is over-fitted, meaning its generalisability and application in individuals not in the cohort will be poor.  To improve the generalisability of the model we now prune the tree by selecting a more permisive value for the complexity parameter (`cp`).  It is recommended that a decision tree is pruned using the Complexity Parameter that corresponds to the minimum cross-validated error (`xerror` in the table below) (@hastie2003 Section 7.10 pg244).  The Complexity Parameter (`cp`) along with associated cross-validated error (`xerror`) is tabulated below and a plot of the two parameters is shown.

```{r results_rpart_categorical_output, echo = FALSE, cache = FALSE, results = 'rmarkdown', eval = TRUE}
categorical$rpart.full.cp %>%
    kable(caption = 'Summary table for over-fitted full model.',
          col.names = c('Complexity Parameter', 'Splits', 'Relative Error', 'Cross-Validated Error', 'Cross-Validated SD'))
plotcp(categorical$rpart.full)

```

The Complexity Parameter (CP) at the step/number of splits (`r categorical$rpart.full.splits.min`) that corresponds to the minimum cross-validated error is `r categorical$rpart.full.cp.min`.

```{r results_rpart_categorical_prune, echo = FALSE, cache = FALSE, results = 'rmarkdown', fig_width = 15, fig_height = 20, eval = TRUE}
prp(categorical$pruned.min,
    type        = prp.opts$type,
    extra       = prp.opts$extra,
    box.palette = prp.opts$box.palette,
    yesno       = prp.opts$yesno,
    branch      = prp.opts$branch,
    varlen      = prp.opts$varlen,
    faclen      = prp.opts$faclen)

```

We now need to calculate the sensitivity and specificity.  There is no single value for either of these metrics since individuals have a predicted probability of classification in the range of $0 < p < 1$ at any given terminal rather than a binary classification.  One approach to viewing the trade-off of the two is to plot the Specificity v's the Sensitivity in what is known as a [Receiver Operating Characteristics (ROC) Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) and calculating the [Area Under the Curve (AUC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) which captures the probability of correctly classifying true positives compared to true negatives.

**IMPORTANT** - Dichotomising individuals predicted probability of disease is required in order to calculate the Sensitivity, Specificity point estimates nor the Positive or Negative Predictive Value.  For now a cut-point of `p = 0.5` has been used, but this is unlikely to be optimal for any of the trees.

```{r results_rpart_categorical_roc_min, echo = FALSE, cache = FALSE, fig_width = 10, fig_height = 10, warnings = FALSE, eval = TRUE}
categorical$roc.min$plot

```

The question arises though whether it is possible to utilise a simpler, more parsimonious tree with less splits, yet still retain the ability to make useful and accurate predictions.  To this end the ROC curve for each step/split in the recursive partitioning process are plotted below and a table of the performance statistics is provided for each step.

```{r results_rpart_categorical_roc_all, echo = FALSE, cache = FALSE, fig_width = 10, fig_height = 10, warnings = FALSE, eval = TRUE}
categorical$roc.all$plot
categorical$roc.all$summary.stats %>%
    kable(caption = paste0('Predictive statistics fo sequential splits of Recursive Partitioning using a cut point of p = ',
                           categorical$threshold,
                           ','))

```
