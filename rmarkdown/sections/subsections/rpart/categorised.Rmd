##### Pre-Categorised Continuous Variables

```{r results_rpart_categorical, echo = FALSE, cache = FALSE, results = 'hide', eval = TRUE}
## Useful reading...
##
## http://gormanalysis.com/decision-trees-in-r-using-rpart/
categorical <- dipep_rpart(df             = dipep,
                           predictor      = c('age.cat', 'smoking', 'temperature.cat', 'bp.diastolic.cat', 'bp.systolic.cat',
                                              'o2.saturation.cat', 'respiratory.rate.cat', 'bmi.cat',
                                              'pregnancies.under.cat', 'pregnancies.over.cat', 'prev.preg.problem',
                                              'presenting.features.pleuritic',
                                              'presenting.features.non.pleuritic', 'presenting.features.sob.exertion',
                                              'presenting.features.sob.rest', 'presenting.features.haemoptysis',
                                              'presenting.features.cough', 'presenting.features.syncope',
                                              'presenting.features.palpitations', 'presenting.features.other',
                                              'surgery', 'cesarean', 'thromb.event',
                                              'thromboprophylaxis', 'thrombosis', 'preg.post', 'num.fetus'),
                           exclude.non.recruited = TRUE,
                           exclude.dvt           = TRUE,
                           legend                = FALSE,
                           threshold             = 0.5,
                           rpart.opts.method     = 'class',
                           rpart.opts.minsplit   = 4,
                           rpart.opts.minbucket  = 2,
                           rpart.opts.cp         = -1)

```

This section shows the results from applying recursive partitioning to pre-categorised continuous variables.  Leave One Out Cross-Validation (LOOCV) is performed automatically by the `rpart` package at each partition and the resulting estimates used in summarising and prunning the tree.



The [dendrogram](https://en.wikipedia.org/wiki/Dendrogram) below shows the full, over-fitted.  As its over-fitted there are a lot of splits and the graph is barely legible.

```{r results_rpart_categorical_plot, echo = FALSE, cache = FALSE, fig_width = 15, fig_height = 10, eval = TRUE}
prp(categorical$rpart.full,
    type        = prp.opts$type,
    extra       = prp.opts$extra,
    box.palette = prp.opts$box.palette,
    yesno       = prp.opts$yesno,
    branch      = prp.opts$branch,
    varlen      = prp.opts$varlen,
    faclen      = prp.opts$faclen)

```


This model, because it was forced to fit a full model that categorised everyone, is over-fitted, meaning its generalisability and application in individuals not in the cohort will be poor.  To improve the generalisability of the model we now prune the tree by selecting a more permisive value for the complexity parameter (`cp`).  It is recommended that a decision tree is pruned using the Complexity Parameter that corresponds to the minimum cross-validated error (`xerror` in the table below) (@hastie2003 Section 7.10 pg244).  The Complexity Parameter (`cp`) along with associated cross-validated error (`xerror`) is tabulated below and a plot of the two parameters is shown.

```{r results_rpart_categorical_output, echo = FALSE, cache = FALSE, results = 'rmarkdown', eval = TRUE}
categorical$rpart.full.cp %>%
    kable(caption = 'Summary table for over-fitted full model.',
          col.names = c('Complexity Parameter', 'Splits', 'Relative Error', 'Cross-Validated Error', 'Cross-Validated SD'))
plotcp(categorical$rpart.full)

```

The Complexity Parameter (CP) at the step/number of splits (`r categorical$rpart.full.splits.min`) that corresponds to the minimum cross-validated error is `r categorical$rpart.full.cp.min`.

```{r results_rpart_categorical_prune, echo = FALSE, cache = FALSE, results = 'rmarkdown', fig_width = 15, fig_height = 20, eval = TRUE}
prp(categorical$pruned.min,
    type        = prp.opts$type,
    extra       = prp.opts$extra,
    box.palette = prp.opts$box.palette,
    yesno       = prp.opts$yesno,
    branch      = prp.opts$branch,
    varlen      = prp.opts$varlen,
    faclen      = prp.opts$faclen)

```

We now need to calculate the sensitivity and specificity.  There is no single value for either of these metrics since individuals have a predicted probability of classification in the range of $0 < p < 1$ at any given terminal rather than a binary classification.  One approach to viewing the trade-off of the two is to plot the Specificity v's the Sensitivity in what is known as a [Receiver Operating Characteristics (ROC) Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) and calculating the [Area Under the Curve (AUC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) which captures the probability of correctly classifying true positives compared to true negatives.

**IMPORTANT** - Until a threshold for dichotomising individuals predicted probability of disease is selected (which can be done based on the ROC Curve) it is not possible to calculate the Sensitivity, Specificity point estimates nor the Positive or Negative Predictive Value.

```{r results_rpart_categorical_roc_min, echo = FALSE, cache = FALSE, fig_width = 10, fig_height = 10, warnings = FALSE, eval = TRUE}
categorical$roc.min$plot

```

The question arises though whether it is possible to utilise a simpler, more parsimonious tree with less splits, yet still retain the ability to make useful and accurate predictions.  To this end the ROC curve for each step/split in the recursive partitioning process are plotted below and a table of the performance statistics is provided for each step.

```{r results_rpart_categorical_roc_all, echo = FALSE, cache = FALSE, fig_width = 10, fig_height = 10, warnings = FALSE, eval = TRUE}
categorical$roc.all$plot
categorical$roc.all$summary.stats %>%
    kable(caption = paste0('Predictive statistics fo sequential splits of Recursive Partitioning using a cut point of p = ',
                           categorical$threshold,
                           ','))

```
