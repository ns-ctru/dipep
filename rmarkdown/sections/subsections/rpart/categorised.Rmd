##### Pre-Categorised Continuous Variables

```{r results_rpart_categorical, echo = FALSE, cache = FALSE, results = 'hide', eval = TRUE}
## Useful reading...
##
## http://gormanalysis.com/decision-trees-in-r-using-rpart/
model <- reformulate(response = 'first.st',
                     termlabels = c('age.cat', 'smoking', 'temperature.cat', 'bp.diastolic.cat', 'bp.systolic.cat',
                                    'o2.saturation.cat', 'respiratory.rate.cat', 'bmi.cat',
                                    'pregnancies.under.cat', 'pregnancies.over.cat', 'prev.preg.problem',
                                    'presenting.features.pleuritic',
                                    'presenting.features.non.pleuritic', 'presenting.features.sob.exertion',
                                    'presenting.features.sob.rest', 'presenting.features.haemoptysis',
                                    'presenting.features.cough', 'presenting.features.syncope',
                                    'presenting.features.palpitations', 'presenting.features.other',
                                    'surgery', 'cesarean', 'thromb.event',
                                    'thromboprophylaxis', 'thrombosis', 'preg.post', 'num.fetus'))
## Test model
categorical <- dplyr::filter(dipep, group %in% c('Diagnosed PE', 'Suspected PE'))
categorical.n.missing <- dplyr::filter(categorical, is.na(first.st)) %>% nrow()
categorical <- rpart(model,
                     data = categorical,
                     method  = rpart.opts$method,
                     control = rpart.control(minsplit  = rpart.opts$minsplit,
                                             minbucket = rpart.opts$minbucket,
                                             ## xval      = nrow(categorical),
                                             cp        = rpart.opts$cp))

```

This section shows the results from applying recursive partitioning.  Leave One Out Cross-Validation (LOOCV) is performed automatically by the `rpart` package at each partition and the resulting estimates used in summarising and prunning the tree.



The [dendrogram](https://en.wikipedia.org/wiki/Dendrogram) below shows the full, over-fitted.  As its over-fitted there are a lot of splits and the graph is barely legible.

```{r results_rpart_categorical_plot, echo = FALSE, cache = FALSE, fig_width = 15, fig_height = 10, eval = TRUE}
prp(categorical,
    type        = prp.opts$type,
    extra       = prp.opts$extra,
    box.palette = prp.opts$box.palette,
    yesno       = prp.opts$yesno,
    branch      = prp.opts$branch,
    varlen      = prp.opts$varlen,
    faclen      = prp.opts$faclen)

```


This model, because it was forced to fit a full model that categorised everyone, is over-fitted, meaning its generalisability and application in individuals not in the cohort will be poor.  To improve the generalisability of the model we now prune the tree by selecting a more permisive value for the complexity parameter (`cp`).  It is recommended that a decision tree is pruned using the Complexity Parameter of the smallest tree within one standard deviation of the smallest reported `xerror` (@hastie2003 Section 7.10 pg244).  The Complexity Parameter (`cp`) is plotted below for each split of the over-fitted full tree.

```{r results_rpart_categorical_output, echo = FALSE, cache = FALSE, results = 'rmarkdown', eval = TRUE}
printcp(categorical)
plotcp(categorical)

```

The horizontal dashed line shows the threshold for selecting the Complexity Parameter to prune the tree, and a value corresponding to the first point **below** this line is selected and applied to the data.

```{r results_rpart_categorical_prune, echo = FALSE, cache = FALSE, results = 'rmarkdown', fig_width = 15, fig_height = 20, eval = TRUE}
## Selecting cp for pruning...
## https://stats.stackexchange.com/questions/122333/interpreting-rpart-output-for-decision-trees?rq=1
cp.table <- categorical$cptable %>%
            as.data.frame() %>%
            mutate(mean = mean(xerror))
cp <- filter(cp.table, CP < mean)[1,1] / 10
categorical_prune <- prune(categorical, cp = cp)
printcp(categorical_prune,
        digits = printcp.opts$digits)
prp(categorical_prune,
    type        = prp.opts$type,
    extra       = prp.opts$extra,
    box.palette = prp.opts$box.palette,
    yesno       = prp.opts$yesno,
    branch      = prp.opts$branch,
    varlen      = prp.opts$varlen,
    faclen      = prp.opts$faclen)

```

We now need to calculate the sensitivity and specificity.  There is no single value for either of these metrics since individuals have a predicted probability of classification in the range of $0 < p < 1$ rather than a binary classification.  One approach to viewing the trade-off of the two is to plot the Specificity v's the Sensitivity in what is known as a [Receiver Operating Characteristics (ROC) Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) and calculating the [Area Under the Curve (AUC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) which captures the probability of correctly classifying true positives compared to true negatives.

**IMPORTANT** - Until a threshold for dichotomising individuals predicted probability of disease is selected (which can be done based on the ROC Curve) it is not possible to calculate the Sensitivity, Specificity point estimates nor the Positive or Negative Predictive Value.

```{r results_rpart_categorical_roc, echo = FALSE, cache = FALSE, fig_width = 10, fig_height = 10, warnings = FALSE, eval = TRUE}
## Extract the predicted values and bind to the observed
## https://stackoverflow.com/questions/31094473/r-how-to-calculate-sensitivity-and-specificity-of-rpart-tree
## xpred.rpart(categorical_prune)
## .predict <- predict(categorical_prune, type = 'prob')
## .predicted <- prediction(.predict[,2] %>% as.vector(), categorical_prune$y)
## pred.obs <- cbind(.predict[,2] %>% as.vector(), categorical_prune$y) %>% as.data.frame()
pred.obs <- cbind(predict(categorical_prune, type = 'prob')[,2],
                  categorical_prune$y) %>%
            as.data.frame()
names(pred.obs) <- c('predicted', 'obs')
roc <- ggplot(pred.obs, aes(d = categorical_prune$y, m = predicted)) +
    geom_roc(n.cuts = 10) + style_roc() +
    ggtitle(paste0('ROC Curve for Pruned Categorical Tree (CP = ', round(cp, 5), ')'))
## Calculate AUC and add to plot
roc + annotate('text', x = 0.75, y = 0.25,
               label = paste0('AUC = ', round(calc_auc(roc)$AUC, 3)))

```

The full summary of the pruned tree is provided.  The `r categorical.n.missing` `observations deleted due to missingness` are those classified as `exclude` rather than to a disease status and not because of missing observations in the predictor variables.

```{r results_rpart_categorical_summary, echo = FALSE, cache = FALSE, results = 'rmarkdown', eval = TRUE}
summary(categorical_prune)

```
