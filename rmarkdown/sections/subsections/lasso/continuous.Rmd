##### Continuous Variables

```{r results_lasso_continuous, echo = FALSE, cache = FALSE, warning = FALSE, message = FALSE, results = 'hide', eval = TRUE}
predictor <- c(continuous, common)
lasso.continuous <- dipep_glmnet_orig(df          = dipep,
                                      classification = classification,
                                      predictor      = predictor,
                                      alpha          = 1,
                                      model          = 'LASSO : Pre-Categorised Variables',
                                      exclude        = NULL,
                                      exclude.non.recruited = TRUE,
                                      exclude.dvt           = TRUE,
                                      exclude.anti.coag     = FALSE,
                                      exclude.missing       = TRUE,
                                      legend                = TRUE,
                                      threshold             = 0.3)


```


```{r results_lasso_continuous_plot, echo = FALSE, cache = FALSE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 10, results = 'asis', eval = TRUE}
## Plot the change in coefficients over time
lasso.continuous$lasso.plot + guides(colour = FALSE)

```

###### Cross-Validation

[Leave One Out Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation) has been utilised internally at each step of the fitting of the LASSO to shrink the point estimate and inform the next iteration.  The model should not be over-fitted though and this can be achieved by considering the parameter Lambda and either taking the minimum value or the value corresponding 1 x SE of the point-estimate of the Mean Squared Error, which are the dashed lines on the following graph.

<!--- Partial explanation at http://stats.stackexchange.com/questions/77546/how-to-interpret-glmnet --->

```{r results_lasso_continuous_cross_validation_plot, echo = FALSE, cache = FALSE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 10, results = 'asis', eval = TRUE}
lasso.continuous$lasso.cv.plot

```

###### ROC Curve

Having selected an optimal value for Lambda the predicted probabilities are then obtained and ROC curves plotted along with the calculated Area Under the Curve (AUC) statistic.  A cut-point for probability can be chosen but in the absence of any choice a default of `p = 0.5` has been used for now in order to calculate the various performance metrics.

```{r results_lasso_continuous_roc, echo = FALSE, cache = FALSE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 10, results = 'asis', eval = TRUE}
lasso.continuous$lasso.cv.roc
lasso.continuous$lasso.cv.summary.stats %>%
    kable(caption = 'Predictive statistics for Lambda Thresholds.')
lasso.continuous$lasso.cv.coef.lambda %>%
    kable(caption = 'Coefficients for Lambda thresholds.')

```

The question naturally arises as to whether a parsimonious model with less variables from an earlier stage in the LASSO can be used without losing predictive ability of the model.  It is self-evident that simpler models will have poorer predictive value because they use less information in order to make the prediction, but in order to provide a visual overview and numerical quantification of the trade-off between complexity and predictive value *all* steps from the LASSO are plotted as ROC curves and goodness of fit statistics have been calculated for each sequential step.


```{r results_lasso_continuous_roc_all, echo = FALSE, cache = FALSE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 10, results = 'asis', eval = TRUE}
lasso.continuous$lasso.cv.roc.all
lasso.continuous$lasso.cv.summary.stats.all %>%
    kable(caption = paste0('Predictive statistics for sequential steps of LASSO using a cut-point of p = ',
                           lasso.continuous$threshold,
                           '.'))

```

```{r results_lasso_continuous_roc_animate, echo = FALSE, cache = FALSE, warning = FALSE, message = FALSE, fig.show = 'animate', eval = FALSE}
## Animation of ROC curve
ani.record(reset = TRUE)
par(bg = 'white')
for(i in seq(1:length(lasso.continuous$lasso.cv$lambda))){
    t <- dplyr::filter(lasso.continuous$lasso.cv.predicted, term == i) %>%
         ggplot(aes(d = D, m = M)) +
         geom_roc() +
         ggtitle(paste0('ROC Curves for LASSO step ', i)) +
        style_roc() + theme_bw()
    print(t)
    ani.record()
}
oopts = ani.options(interval = 0.5)
ani.replay()
rm(t)

```
