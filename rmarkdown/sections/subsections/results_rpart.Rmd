### Recursive Partitioning

```{r results_rpart_opts, echo = FALSE, cache = FALSE, results = 'hide', eval = TRUE}
## Rpart options
rpart.opts           <- list()
rpart.opts$method    <- 'class'
rpart.opts$minsplit  <- 2
rpart.opts$minbucket <- 2
rpart.opts$cp        <- -1
printcp.opts         <- list()
printcp.opts$digits  <- 5
## prp options
prp.opts <- list()
prp.opts$type        <- 2
prp.opts$extra       <- 'auto'
prp.opts$box.palette <- c('green', 'red')
prp.opts$yesno       <- 1
prp.opts$branch      <- 1
prp.opts$varlen      <- 0
prp.opts$faclen      <- 0

```

[Recursive Partitioning](https://en.wikipedia.org/wiki/Recursive_partitioning) is a method of automatically selecting variables, and when continuous cut-points within the range of a given variable, that maximise the classification of individuals.  The [R](https://www.r-project.org/) package [rpart](https://cran.r-project.org/web/packages/rpart/) has been used to fit a model using recursive partitioning.  A simple overview of the procedure is described [here](http://statmethods.net/advstats/cart.html).  If you are interested in learning about the this method it is *highly* recommended you read the packages [vignette](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf).

**Caution** Recursive partitioning is normally used on large datasets (i.e. N is in the tens of thousands).  Frank Harrell (author of *Regression Modeeling Strategies*@harrell2006) notes that with $<20000$ observations the results of recursive partitioning are unreliable i.e. repeated bootstraps results in different trees) (see his comment [here](http://stats.stackexchange.com/a/13461/25629)).  The Dipep study is far from being anywhere near this sample size and whilst cross-validation is utilized (automatically) when developing

#### Model

The model that has been fitted uses the primary classification of individuals from UKOSS and those identified as Suspected PE.  A large number of predictor variables are used initially (although in doing so caution should be used in interpretting the results because the cohort is quite small in the context of fitting such models).  The model takes the form...

$$
pulmonary.embolism ~ all.covariates
$$

There are a number of control parameters that are used, these are described in the table below.

Table: Control parameters used in fitting models with `rpart`

Control Parameter | Value     | Explanation
------------------|-----------|---------------------------------------------------------
`minsplit`        | `r rpart.opts$minsplit`  | The minimum number of observations that must exist in a node in order for a split to be attempted.
`minbucket`       | `r rpart.opts$minbucket` | The mimimum number of observations in any terminal node.
`cp`              | `r rpart.opts$cp`        | The Complexity Parameter, a negative value ensures a full model is fitted where everyone is classified.

#### Results  {.tabset .tabset-fade .tabset-pills}

Dendrograms are a simple and natural method of visualising the results of recursive partitioning and are created using the `rpart.plot` (@rpart.plotpackage).  For those unfamiliar with them for each node (split) the variable is listed on the left along with the threshold, the same side as the consequences of a positive repsonse to this variable.  Each leaf (the result of a split) has a box which indicates the probability of the second class and the percentage of observations within this classification.  Currently the graphs are crude and use the variable names used in the Prospect database which do not always align with clear informative descriptions but should be sufficiently useful.  This can be addressed when preparing figures for publication but will not be altered in this report as there is insufficient time to spend tweaking such visual

```{r child = 'rpart/categorised.Rmd', eval = TRUE}
```

```{r child = 'rpart/continuous.Rmd', eval = TRUE}
```

```{r child = 'rpart/staged.Rmd', eval = TRUE}
```
