### Recursive Partitioning

```{r results_rpart_opts, echo = FALSE, cache = FALSE, results = 'hide', eval = TRUE}
## Rpart options
rpart.opts           <- list()
rpart.opts$method    <- 'class'
rpart.opts$minsplit  <- 2
rpart.opts$minbucket <- 2
rpart.opts$cp        <- -1
## prp options
prp.opts <- list()
prp.opts$type        <- 2
prp.opts$extra       <- 'auto'
prp.opts$box.palette <- c('green', 'red')
prp.opts$yesno       <- 1
prp.opts$branch      <- 1
prp.opts$varlen      <- 0
prp.opts$faclen      <- 0

```

```{r results_rpart_categorical, echo = FALSE, cache = FALSE, results = 'hide', eval = TRUE}
## Useful reading...
##
## http://gormanalysis.com/decision-trees-in-r-using-rpart/
model <- reformulate(response = 'first.st',
                     termlabels = c('age', 'smoking', 'temperature.cat', 'bp.diastolic.cat', 'bp.systolic.cat',
                                    'o2.saturation.cat', 'respiratory.rate.cat', 'bmi.cat',
                                    'pregnancies.under.cat', 'pregnancies.over.cat', 'prev.preg.problem',
                                    'presenting.features.pleuritic',
                                    'presenting.features.non.pleuritic', 'presenting.features.sob.exertion',
                                    'presenting.features.sob.rest', 'presenting.features.haemoptysis',
                                    'presenting.features.cough', 'presenting.features.syncope',
                                    'presenting.features.palpitations', 'presenting.features.other',
                                    'surgery', 'cesarean', 'thromb.event',
                                    'thromboprophylaxis', 'thrombosis', 'preg.post', 'num.fetus'))
## Test model
categorical <- rpart(model,
                data = filter(dipep, group %in% c('UKOSS', 'Suspected PE')),
                method = rpart.opts$method,
                minsplit = rpart.opts$minsplit,
                minbucket = rpart.opts$minbucket,
                cp = rpart.opts$cp)    ## NB - Make sure 'pe' is a factor!

```
```{r results_rpart_continuous, echo = FALSE, cache = FALSE, results = 'hide', eval = TRUE}
## Repeat using continuous variables
model <- reformulate(response = 'first.st',
                     termlabels = c('age', 'smoking', 'temperature', 'bp.diastolic', 'bp.systolic',
                                    'o2.saturation', 'respiratory.rate', 'bmi',
                                    'pregnancies.under', 'pregnancies.over', 'prev.preg.problem',
                                    'presenting.features.pleuritic',
                                    'presenting.features.non.pleuritic', 'presenting.features.sob.exertion',
                                    'presenting.features.sob.rest', 'presenting.features.haemoptysis',
                                    'presenting.features.cough', 'presenting.features.syncope',
                                    'presenting.features.palpitations', 'presenting.features.other',
                                    'surgery', 'cesarean', 'thromb.event',
                                    'thromboprophylaxis', 'thrombosis', 'preg.post', 'num.fetus'))
## Test model
continuous <- rpart(model,
              data = filter(dipep, group %in% c('UKOSS', 'Suspected PE')),
              method = rpart.opts$method,
              minsplit = rpart.opts$minsplit,
              minbucket = rpart.opts$minbucket,
              cp = rpart.opts$cp)    ## NB - Make sure 'pe' is a factor!

```

```{r results_rpart_save, echo = FALSE, cache = FALSE, results = 'hide', eval = FALSE}
save(continuous, categorical,
     file = '../../data/results/rpart.RData')

```
```{r results_rpart_load, echo = FALSE, cache = FALSE, results = 'hide', eval = FALSE}
load(file = '../../data/results/rpart.RData')

```

[Recursive Partitioning](https://en.wikipedia.org/wiki/Recursive_partitioning) is a method of automatically selecting variables, and when continuous cut-points within the range of a given variable, that maximise the classification of individuals.  The [R](https://www.r-project.org/) package [rpart](https://cran.r-project.org/web/packages/rpart/) has been used to fit a model using recursive partitioning and if you are interested in learning about the this method it is recommended you read the packages [vignette](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf).

**Caution** Recursive partitioning is normally used on large datasets (i.e. N is in the tens of thousands).  Frank Harrell (author of *Regression Modeeling Strategies*@harrell2006) notes that with $<20000$ observations the results of recursive partitioning are unreliable i.e. repeated bootstraps results in different trees) (see his comment [here](http://stats.stackexchange.com/a/13461/25629)).

#### Model

The model that has been fitted uses the primary classification of individuals from UKOSS and those identified as Suspected PE.  A large number of predictor variables are used initially (although in doing so caution should be used in interpretting the results because the cohort is quite small in the context of fitting such models).  The model takes the form...

$$
pulmonary.embolism ~ all.covariates
$$

There are a number of control parameters that are used, these are described in the table below.

Table: Control parameters used in fitting models with `rpart`

Control Parameter | Value     | Explanation
------------------|-----------|---------------------------------------------------------
`minsplit`        | `r rpart.opts$minsplit`  | The minimum number of observations that must exist in a node in order for a split to be attempted.
`minbucket`       | `r rpart.opts$minbucket` | The mimimum number of observations in any terminal node.
`cp`              | `r rpart.opts$cp`        | The Complexity Parameter, a negative value ensures a full model is fitted where everyone is classified.

#### Results  {.tabset .tabset-fade .tabset-pills}

Dendrograms are a simple and natural method of visualising the results of recursive partitioning and are created using the `rpart.plot` (@rpart.plotpackage).  For those unfamiliar with them for each node (split) the variable is listed on the left along with the threshold, the same side as the consequences of a positive repsonse to this variable.  Each leaf (the result of a split) has a box which indicates the probability of the second class and the percentage of observations within this classification.  Currently the graphs are crude and use the variable names used in the Prospect database which do not always align with clear informative descriptions but should be sufficiently useful.  This can be addressed when preparing figures for publication but will not be altered in this report as there is insufficient time to spend tweaking such visual

##### Pre-Categorised Continuous Variables

The results below are from applying recursive partitioning per-protocol approach of pre-dichotomised covariates.

Cross-validation is performed automatically by the `rpart` package and is summarised below.  The importance of each variable is listed first, which is the sum of the goodness of split measures for each split for which it is the primary variable.  The cross-validation results are then shown.  The  Complexity Parameter (`cp`) **TODO** finish off explanation.

```{r results_rpart_categorical_output, echo = FALSE, cache = FALSE, results = 'rmarkdown', eval = TRUE}
printcp(categorical)

```

The positive response to the split is always on the left-hand side of a node, the numbers in the boxes.  The dendrogram has been augmented with additional information showing

```{r results_rpart_categorical_plot, echo = FALSE, cache = FALSE, fig_width = 15, fig_height = 10, eval = TRUE}
prp(categorical,
    type        = prp.opts$type,
    extra       = prp.opts$extra,
    box.palette = prp.opts$box.palette,
    yesno       = prp.opts$yesno,
    branch      = prp.opts$branch,
    varlen      = prp.opts$varlen,
    faclen      = prp.opts$faclen)

```

This model, because it was forced to fit a full model that categorised everyone, is over-fitted, meaning its generalisability and application in individuals not in the cohort will be poor.  To improve the generalisability of the model we now prune the tree by selecting a more permisive value for the complexity parameter (`cp`).

```{r results_rpart_categorical_prune, echo = FALSE, cache = FALSE, results = 'rmarkdown', fig_width = 15, fig_height = 10, eval = TRUE}
cp <- 0.022222
categorical_prune <- prune(categorical, cp = cp)
printcp(categorical_prune)
prp(categorical_prune,
    type        = prp.opts$type,
    extra       = prp.opts$extra,
    box.palette = prp.opts$box.palette,
    yesno       = prp.opts$yesno,
    branch      = prp.opts$branch,
    varlen      = prp.opts$varlen,
    faclen      = prp.opts$faclen)

```

We now need to calculate the sensitivity and specificity.  There is no single value since the cut-point for classifying can vary depending on the complexity value we wish to use, i.e. do we favour sensitivity over specificity or vice-versa.  One approach to viewing the trade-off of the two is to plot the Specificity v's the Sensitivity in what is known as a [Receiver Operating Characteristics (ROC) Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).

```{r results_rpart_categorical_roc, echo = FALSE, cache = FALSE, fig_width = 10, fig_height = 10, warnings = FALSE, eval = TRUE}
## Extract the predicted values and bind to the observed
## https://stackoverflow.com/questions/31094473/r-how-to-calculate-sensitivity-and-specificity-of-rpart-tree
## xpred.rpart(categorical_prune)
## .predict <- predict(categorical_prune, type = 'prob')
## .predicted <- prediction(.predict[,2] %>% as.vector(), categorical_prune$y)
## pred.obs <- cbind(.predict[,2] %>% as.vector(), categorical_prune$y) %>% as.data.frame()
pred.obs <- cbind(predict(categorical_prune, type = 'prob')[,2],
                  categorical_prune$y) %>%
            as.data.frame()
names(pred.obs) <- c('predicted', 'obs')
roc <- ggplot(pred.obs, aes(d = categorical_prune$y, m = predicted)) +
    geom_roc(n.cuts = 10) + style_roc() +
    ggtitle(paste0('ROC Curve for Pruned Categorical Tree (CP = ', cp, ')'))
## Calculate AUC and add to plot
roc + annotate('text', x = 0.75, y = 0.25,
               label = paste0('AUC = ', round(calc_auc(roc)$AUC, 3)))

```


##### Continuous Variables

Because recursive partitioning seeks to optimaise the cut-point for continuous variables it is perhaps sub-optimal to pre-specify cut-points aprior to testing the model.  To this end the models have been tested using the raw underlying continuous data (which contains more fine-grained information than the dichotomised variables) and the models re-run allowing the algorithms to choose cut-points for partitioning.

Cross-validation is performed automatically by the `rpart` package and is summarised below.  The importance of each variable is listed first, which is the sum of the goodness of split measures for each split for which it is the primary variable.  The cross-validation results are then shown.  The  Complexity Parameter (`cp`)

```{r results_rpart_continuous_output, echo = FALSE, cache = FALSE, results = 'rmarkdown', eval = TRUE}
printcp(continuous)

```
The positive response to the split is always on the left-hand side of a node, the numbers in the boxes

```{r results_rpart_continuous_plot, echo = FALSE, cache = FALSE, fig_width = 15, fig_height = 10, eval = TRUE}
prp(continuous,
    type        = prp.opts$type,
    extra       = prp.opts$extra,
    box.palette = prp.opts$box.palette,
    yesno       = prp.opts$yesno,
    branch      = prp.opts$branch,
    varlen      = prp.opts$varlen,
    faclen      = prp.opts$faclen)

```



In addition to the problem described for the categorical tree whereby overfitting arises due to forcing a fully fitted tree which then needs trimming, continuousy fitted partition trees also tend to be over-fitted, which makes their generalisability and utility in other, as yet unobserved datasets, severly limited because cut-points are based on the observed data which is only a sample of the target population.  To this end the fitted trees are again pruned to avoid this.  It is recommended that a decision tree is pruned using the Complexity Parameter of the smallest tree within one standard deviation of the smallest reported `xerror`.

```{r results_rpart_continuous_prune, echo = FALSE, cache = FALSE, fig_width = 15, fig_height = 10, results = 'rmarkdown', eval = TRUE}
cp <- 0.0000
continuous_prune <- prune(continuous, cp = cp)
printcp(continuous_prune)
prp(continuous_prune,
    type        = prp.opts$type,
    extra       = prp.opts$extra,
    box.palette = prp.opts$box.palette,
    yesno       = prp.opts$yesno,
    branch      = prp.opts$branch,
    varlen      = prp.opts$varlen,
    faclen      = prp.opts$faclen)

```

```{r results_rpart_continuous_roc, echo = FALSE, cache = FALSE, fig_width = 10, fig_height = 10, warnings = FALSE, eval = TRUE}
## .predict <- predict(continuous_prune, type = 'prob')
## .predicted <- prediction(.predict[,2] %>% as.vector(), continuous_prune$y)
## pred.obs <- cbind(.predict[,2] %>% as.vector(), continuous_prune$y) %>% as.data.frame()
pred.obs <- cbind(predict(continuous_prune, type = 'prob')[,2],
                  continuous_prune$y) %>%
            as.data.frame()
names(pred.obs) <- c('predicted', 'obs')
roc <- ggplot(pred.obs, aes(d = obs, m = predicted)) +
       geom_roc(n.cuts = 10) + style_roc() +
       ggtitle(paste0('ROC Curve for Pruned Continuous Tree (CP = ', cp, ')'))
    ## Calculate AUC and add to plot
roc + annotate('text', x = 0.75, y = 0.25,
               label = paste0('AUC = ', round(calc_auc(roc)$AUC, 3)))

```
